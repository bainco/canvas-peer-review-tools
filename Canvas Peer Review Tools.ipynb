{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba79f57f",
   "metadata": {},
   "source": [
    "# Canvas Peer Review Tools\n",
    "\n",
    "Welcome! If you're here, you probably want to use Canvas's built-in Peer Feedback tools but on an assignment that doesn't necessarily look like an essay. Here we've put some helper methods\n",
    "\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "To get started you'll need:\n",
    "\n",
    "* a Canvas API Key ([instructions to generate that can be found here](https://kb.iu.edu/d/aaja))\n",
    "* your Canvas URL (e.g. https://canvas.northwestern.edu)\n",
    "* your Course ID (the easiest way to do this is to navigate to your course's Canvas page and copy the 6-digit number at the end of the URL)\n",
    "* a working Python installation (which you probably have if you're in this notebook)\n",
    "* to install the `canvasapi` from https://github.com/ucfopen/canvasapi\n",
    "* to install the `jinja2` templating package\n",
    "\n",
    "## Why is any of this Necessary?\n",
    "\n",
    "Canvas's built-in Peer Feedback tools work just fine for essays (think something that fits in a PDF, DOCX, etc.) but not so much for files that require downloading (like a `.py` file). Additionally, though you can assign a rubric to an assignment, it's not necessarilly a particular readable format for the peer getting the feedback. To fix that, we've got some really basic tools here that communicate with Canvas directly via their API.\n",
    "\n",
    "## What does this assume?\n",
    "\n",
    "It assumes that you've followed Canvas's instructions to assign a required Peer Review of a particular assignment in the course. [Instructions on how to do that are here](https://community.canvaslms.com/t5/Instructor-Guide/How-do-I-use-peer-review-assignments-in-a-course/ta-p/697).\n",
    "\n",
    "Unfortunately, there's one manual step here (it can be solved programmatically...but it depends on the exact details of your class).\n",
    "\n",
    "By default, Canvas will NOT assign a peer review to anyone who didn't complete the assignment themselves. You can override this using Canvas's web interface (i.e. assign them a peer review) **but they won't actually be able to complete it.**\n",
    "\n",
    "To solve this, you can simply _submit on the student's behalf_. For instance, I just created a blank `.py` file, went through all of the students who did not submit this homework, and then used [Canvas's new \"Submit for Student\" functionality](https://community.canvaslms.com/t5/Instructor-Guide/How-do-I-submit-an-assignment-on-behalf-of-a-student-as-an/ta-p/560948).\n",
    "\n",
    "In fact, if you do this BEFORE assigning the Peer Reviews, it will all happen like it's supposed to!\n",
    "\n",
    "If you want to solve this programmatically you can. I'll include my code for doing it near the bottom of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ce93a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to install the jinja2 templating system? Run this cell!\n",
    "%pip install jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbca6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to install the Canvas API Python Wrapper? Run this cell!\n",
    "%pip install canvasapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2ce7fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where you can specify your Canvas info\n",
    "canvas_token = \"YOUR_API_TOKEN_GOES_HERE\"\n",
    "course_id = 123456\n",
    "canvas_url = \"https://canvas.cool-place.edu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b2c185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These will be the main instances of your Canvas connection and course\n",
    "# More info in the canvasapi docs: https://canvasapi.readthedocs.io/en/stable/\n",
    "canvas = Canvas(canvas_url, canvas_token)\n",
    "current_course = canvas.get_course(course_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ff6271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from canvasapi.submission import Submission\n",
    "from canvasapi.upload import FileOrPathLike, Uploader\n",
    "\n",
    "# Temporary fix to make sure comments/feedback go to latest submission\n",
    "#   and so you can post file_commments with custom text\n",
    "#   Have submitted PR to ucfopen/canvasapi to get this fixed\n",
    "def upload_comment(self, file: FileOrPathLike, **kwargs):\n",
    "    response = Uploader(\n",
    "        self._requester,\n",
    "        \"courses/{}/assignments/{}/submissions/{}/comments/files\".format(\n",
    "            self.course_id, self.assignment_id, self.user_id\n",
    "        ),\n",
    "        file,\n",
    "        **kwargs,\n",
    "    ).start()\n",
    "\n",
    "    if response[0]:\n",
    "        if \"comment\" in kwargs:\n",
    "            kwargs[\"comment\"] |= {\"file_ids\": [response[1][\"id\"]]}\n",
    "        else:\n",
    "            kwargs[\"comment\"] = {\"file_ids\": [response[1][\"id\"]]}\n",
    "\n",
    "        self.edit(**kwargs)\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "# monkey patching built-in Submission.upload_comment with the new version\n",
    "Submission.upload_comment = upload_comment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233919a2",
   "metadata": {},
   "source": [
    "In my class, students were doing a peer review of one assignment and it was counting for a grade on another assignment. Specifically:\n",
    "\n",
    "* HW 3 - The assignment that was going to be peer reviewed\n",
    "* HW 4 - The assignment which contained review instructions and would hold the grade for whether or not the student completed the review\n",
    "\n",
    "So first, we need to get the `ID`s for each of these assignments (a unique numeric code that identifies those assignments in Canvas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c22cd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to see all assignments in your Canvas course\n",
    "# Note, this does NOT include Quizzes as those are not technically Assignments\n",
    "all_assignments = current_course.get_assignments()\n",
    "\n",
    "for an_assignment in all_assignments:\n",
    "    print(an_assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c5a437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're following a similar structure to my setup, insert your assignment ids here\n",
    "hw3 = current_course.get_assignment(ASSIGNMENT_ID_FOR_HW3)\n",
    "hw4 = current_course.get_assignment(ASSIGNMENT_ID_FOR_HW4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed12db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hw4_subs = {}\n",
    "for sub in hw4.get_submissions():\n",
    "    if sub.grade:\n",
    "        print('already graded')\n",
    "    else:\n",
    "        hw4_subs[str(sub.id)] = sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7017443",
   "metadata": {},
   "outputs": [],
   "source": [
    "hw4_subs[str(43099227)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e84747",
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = hw3.get_submissions(include=\"peer_assessments\", style=\"full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cd2054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfortunately, the Canvas feedback interface doesn't allow a student to download the other's submission. \n",
    "#   To fix this, we just post that student's submission (with names removed) as a file comment on their reviewer's submission.\n",
    "\n",
    "# Keep track of who's been taken care of already just in case we mess up partway\n",
    "done = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4befb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each peer review\n",
    "for item in hw3.get_peer_reviews(include=\"submission_comments\"):\n",
    "    print(item.assessor_id, item.user_id, item.workflow_state) # progress statement\n",
    "    \n",
    "    # Double check we haven't taken care of this one already\n",
    "    if item.assessor_id in done:\n",
    "        print(\"Skipping!\")\n",
    "        continue\n",
    "        \n",
    "    # Get the reviewee and the reviewer\n",
    "    reviewee = current_course.get_user(item.user_id)\n",
    "    reviewer = current_course.get_user(item.assessor_id)\n",
    "    \n",
    "    # Get the submission for HW 3 for the reviewee\n",
    "    sub_to_be_reviewed = hw3.get_submission(item.user_id)\n",
    "    \n",
    "    # Get the submission for HW 4 for the reviewer\n",
    "    sub_for_reviewer = hw4.get_submission(item.assessor_id)\n",
    "    # Check to see if they have any attachments as part of their submission\n",
    "    if hasattr(sub_to_be_reviewed, \"attachments\"):\n",
    "        if len(sub_to_be_reviewed.attachments) > 1:\n",
    "            print(\"MULTIPLE ATTACHMENTS WARNING\")\n",
    "        for i in range(len(sub_to_be_reviewed.attachments)):\n",
    "            filename = f\"hw3_peer_review_for_{reviewer.name.replace(' ', '_')}.py\"\n",
    "            \n",
    "            # Actually download the submission\n",
    "            try:\n",
    "                print(f\"    Downloading...{filename}\")\n",
    "                sub_to_be_reviewed.attachments[i].download(filename)\n",
    "                \n",
    "                # Create a comment to post to the reviewer's HW 4\n",
    "                comment_dict = {\n",
    "                    \"attempt\": sub_for_reviewer.attempt,\n",
    "                    \"text_comment\": \n",
    "                        \"Here's a link to the file for your peer's submission! You can download this file and put it in the same folder as your own HW3 to run it and see your peer's creature!\"\n",
    "                }\n",
    "                \n",
    "                # Upload the reviewee's submission to the reviewer's HW4.\n",
    "                sub_for_reviewer.upload_comment(filename, comment=comment_dict)\n",
    "                print(f\"Uploaded {filename} for {reviewer.name} who's peer reviewing {reviewee.name}\")\n",
    "                done.append(reviewer.id)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(\"    Couldn't download assignment: {}\".format(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09abed7",
   "metadata": {},
   "source": [
    "## What's happened so far?\n",
    "\n",
    "So at this point, we've \"posted\" everyone's HW 3 submission as a file comment on their reviewers HW 4 submission! Why is this important? Because now each reviewer has download access to each reviewee's file!\n",
    "\n",
    "## What next?\n",
    "\n",
    "Well, now you wait for everyone to complete their Peer Review! Some caveats:\n",
    "\n",
    "* Canvas **does not** timestamp Peer Reviews in any way that we have access to\n",
    "* If you want to set a deadline, then you'll need to grade these reviews right at the deadline\n",
    "* If you want to allow late peer reviews, then you'll have to track those \"manually\"\n",
    "\n",
    "## What now?\n",
    "\n",
    "Alright so everyone's completed their review. How do you...\n",
    "* Grade it (maybe with some requirements on the comments)\n",
    "* Make it available to the Peer (beyond the default Canvas rubric which will)\n",
    "* Make a copy of it available to the reviewer\n",
    "\n",
    "Well, let's do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94e9481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inside a Peer Review stored in Canvas, we have a mapping of \"reviewer\" to \"reviewee\".\n",
    "#   While that's inside the peer review, it's NOT inside the actual \"rubric assessment\" object\n",
    "#   that gets affiliated with each student's submission. So we save these relations in a \"map\"\n",
    "#   which is just a dictionary mapping a reviewer's id to their reviewee's id.\n",
    "reviewer_to_reviewee = {}\n",
    "for item in hw3.get_peer_reviews():\n",
    "    reviewer_to_reviewee[str(item.assessor_id)] = item.user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cbcc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're using a rubric, we need to find its id. This cell will just print out\n",
    "#   all of the rubrics with their names and ids so you can identify the one you want.\n",
    "rubrics = current_course.get_rubrics()\n",
    "for rubric in rubrics:\n",
    "    print(rubric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c1cf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grab the specific rubric you found\n",
    "canvas_rubric = current_course.get_rubric(RUBRIC_ID_GOES_HERE, include=\"peer_assessments\", style=\"full\")\n",
    "\n",
    "# This fetches the raw Canvas rubric which is documented here:\n",
    "#   https://canvas.instructure.com/doc/api/rubrics.html\n",
    "\n",
    "# It then loops through that rubric and creates a more Pythonic version of it to be used later\n",
    "py_rubric = {}\n",
    "for rubric_item in canvas_rubric.data:\n",
    "    py_rubric[rubric_item['id']] = {'description': rubric_item['description'], 'long_description': rubric_item['long_description']}\n",
    "    \n",
    "## Have to map reviewers and reviewees cause the way Python stores the rubrics\n",
    "reviewer_to_reviewee = {}\n",
    "for item in hw3.get_peer_reviews():\n",
    "    reviewer_to_reviewee[str(item.assessor_id)] = item.user_id\n",
    "\n",
    "# Load in the peer review HTML template (a prettier version of the Canvas rubric)\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "environment = Environment(loader=FileSystemLoader(\"./\"))\n",
    "template = environment.get_template(\"templates/peer_review_template.html\")\n",
    "\n",
    "# Go through every \"rubric assessment\" (peer review)\n",
    "for pr in canvas_rubric.assessments:    \n",
    "    rubric = {}\n",
    "    \n",
    "    comment_penalty = 0\n",
    "    \n",
    "    # look up the reviewer and reviewee (using that mapping we created earlier)\n",
    "    reviewer = current_course.get_user(pr['assessor_id'])\n",
    "    reviewee = current_course.get_user(reviewer_to_reviewee[str(pr['assessor_id'])])\n",
    "\n",
    "    # load their submissions, HW3 for the reviewee; HW4 for the reviewer; and HW4 for the reviewee\n",
    "    sub_that_was_reviewed = hw3.get_submission(reviewee.id)\n",
    "    sub_for_reviewer = hw4.get_submission(reviewer.id)\n",
    "    reviewee_hw4_sub = hw4.get_submission(reviewee.id)\n",
    "  \n",
    "    # Check to make sure this reviewer hasn't already received a grade\n",
    "    if sub_for_reviewer.workflow_state == \"graded\":\n",
    "        print(\"Already graded!\")\n",
    "        continue\n",
    "    \n",
    "    # Process the reviewer's  review\n",
    "    for item in pr['data']:\n",
    "        \n",
    "        # Data fix for \"blank\" comments\n",
    "        if item['comments'] is None:\n",
    "            item['comments'] = \"\"\n",
    "            \n",
    "        \n",
    "        ## Here's where you would place \"expectations\" for reviewer comments.\n",
    "        ##  For instance, I asked that reviewers add at least two sentences to each\n",
    "        ##  rubric item justifying their choice. I check for this by expecting each\n",
    "        ##  rubric item to have at least 10 words in its comment property.\n",
    "        ##  You can implement anything here – even expectations on word usage, piping it\n",
    "        ##  through an LLM or other automated language model (simple or advanced)\n",
    "            \n",
    "        # Do word count check\n",
    "        word_count = len(re.findall(r'\\w+', item['comments']))\n",
    "        \n",
    "        # If the word count is below the expectation, they didn't meet the expectations\n",
    "        if word_count < 10:\n",
    "            comment_penalty += 1\n",
    "            \n",
    "        rubric[py_rubric.get(item['criterion_id'])['description']] = {'description': item['description'], \n",
    "                                                                      'long_description': py_rubric.get(item['criterion_id'])['long_description'], \n",
    "                                                                      'comments':item['comments'], \n",
    "                                                                      'class': 'warning' if item['description'] == \"Room for Improvement\" else 'success'}    \n",
    "    # Everyone starts with the max grade\n",
    "    grade = hw4.points_possible\n",
    "    # If they received comment penalties, their grade goes down.\n",
    "    # In my case, I spotted everyone \"1\" comment penalty\n",
    "    if comment_penalty > 1:\n",
    "        grade = grade - (comment_penalty - 1) * 10\n",
    "        \n",
    "    # Output to the screen who gets what (this is all local so deanonymized).\n",
    "    print(f\"{reviewer.short_name} will receive a {grade} for their review of {reviewee.short_name}\")\n",
    "    \n",
    "    # Generate anonymous file names for the reviewer (who gets a copy) and the reviewee\n",
    "    file_for_reviewer = f\"HW3_Peer_Review_by_{reviewer.short_name.replace(' ', '_')}.html\"\n",
    "    file_for_reviewee = f\"HW3_Peer_Review_for_{reviewee.short_name.replace(' ', '_')}.html\"\n",
    "    \n",
    "    # Render the two peer reviews in our nice template\n",
    "    with open(file_for_reviewer, mode=\"w\", encoding=\"utf-8\") as report:\n",
    "        content = template.render(\n",
    "                title= file_for_reviewer[:-5].replace(\"_\", \" \"),\n",
    "                student_name=reviewer.short_name,\n",
    "                netid=reviewer.sis_user_id,\n",
    "                rubric=rubric\n",
    "                )\n",
    "        report.write(content)\n",
    "    \n",
    "    with open(file_for_reviewee, mode=\"w\", encoding=\"utf-8\") as report:\n",
    "        content = template.render(\n",
    "                title= file_for_reviewee[:-5].replace(\"_\", \" \"),\n",
    "                student_name=reviewee.short_name,\n",
    "                netid=reviewee.sis_user_id,\n",
    "                rubric=rubric\n",
    "                )\n",
    "        report.write(content)\n",
    "    \n",
    "\n",
    "    # GRADE Peer Review for REVIEWER AND POST REVIEWER FILE    \n",
    "    sub_for_reviewer.upload_comment(\n",
    "                file_for_reviewer,\n",
    "                submission={\"posted_grade\": grade},\n",
    "                comment={\n",
    "                    \"attempt\": sub_for_reviewer.attempt,\n",
    "                    \"text_comment\": \"Thanks for submitting a HW 3 Peer Review! Here's the contents of your review:\"\n",
    "                },\n",
    "            )\n",
    "    \n",
    "    \n",
    "    # POST Reviewee FILE to REVIEWEE SUB\n",
    "    reviewee_hw4_sub.upload_comment(\n",
    "                file_for_reviewee,\n",
    "                comment={\n",
    "                    \"attempt\": reviewee_hw4_sub.attempt,\n",
    "                    \"text_comment\": \"Here's a nicer version of your Peer's Review of your HW 3 submission!\",\n",
    "                },\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85adecff",
   "metadata": {},
   "source": [
    "## That's it!\n",
    "\n",
    "That's basically it. Not so bad! Frustrating you have to do all of that yourself, but oh well. \n",
    "\n",
    "\n",
    "## Issues\n",
    "\n",
    "So...as in all things...there are some issues here.\n",
    "\n",
    "* Canvas Peer Reviews, once submitted, are uneditable.\n",
    "\n",
    "If you want students to be able to edit their reviews, resubmit, etc., you'll have to do that through some other mechanism. While the Canvas Web interface allows you to \"unassign\" a peer review...it doesn't actually delete the person's \"rubric assessment\" meaning if you assign them the same peer to review...you've made no progress towards resetting the peer review! So this means you need to manually go in and delete the rubric association, something the `canvasapi` library does not support so you need to make a custom request. More details below.\n",
    "\n",
    "* Students don't follow instructions. What do you do if they put the comments not in the rubric but the assignment itself?\n",
    "\n",
    "Good question to which I do not have an answer. If you take a look at my instructions, I even included a screenshot of how to add comments in the rubric...and yet.\n",
    "\n",
    "Additionally, it's worth noting that comments added to the SUBMISSION rather than the RUBRIC are NOT anonymous. So there's so identity leakage here. In my class, I decided it wasn't worth the fight to take points off for this and just reviewed these manually (i.e. anyone who didn't get a 100, I looked at in the Canvas web interface to see what they did wrong).\n",
    "\n",
    "It would be possible to use the Canvas API to parse an overall comment, delete it, and move it into a rubric, but it would be non-trivial. I don't think it's worth the time necessary to figure out when you can just tell the student to do it again (use the delete steps below) or assign them a penalty for not following the instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42fac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You'll need the reviewer's Canvas ID. You can find that via the API, or you can go to that\n",
    "#  person's Canvas profile in your course and grab it from the URL.\n",
    "REIVEWER_ID_TO_DELETE = 1234567\n",
    "\n",
    "## THIS WILL DELETE A RUBRIC ASSESSMENT\n",
    "pr_rubric = current_course.get_rubric(RUBRIC_ID, include=[\"peer_assessments\"])\n",
    "for i in pr_rubric.assessments:\n",
    "    if i['assessor_id'] == REIVEWER_ID_TO_DELETE:\n",
    "        print(\"found reviewer's rubric sumission\")\n",
    "        break\n",
    "        \n",
    "# Interestingly...I've gotten some odd behavior doing this. Sometimes it seems to destroy the peer review entirely\n",
    "#.  meaning you have to go and assign the review again.\n",
    "print(f\"courses/{course_id}/rubric_associations/{i['rubric_association_id']}/rubric_assessments/{i['id']}\")\n",
    "pr_rubric._requester.request(\"DELETE\", f\"courses/{course_id}/rubric_associations/{i['rubric_association_id']}/rubric_assessments/{i['id']}\")\n",
    "\n",
    "# Now go see if you need to go re-assign the peer review. If you do, here's their details:\n",
    "reviewer = current_course.get_user(REIVEWER_ID_TO_DELETE)\n",
    "peer = current_course.get_user(reviewer_to_reviewee.get(str(REIVEWER_ID_TO_DELETE)))\n",
    "print(reviewer.short_name, \"needs to be re-assigned to review\", peer.short_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d54520b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
